# -*- coding: utf-8 -*-
"""Amazon audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVig4YqkODTUnYYuHWkRYAIRUPfkOgld
"""

!pip -q install boto3 s3fs pyarrow pandas pyspark==3.5.1 sqlalchemy psycopg2-binary

import os, json, uuid, datetime as dt
import pandas as pd
import numpy as np

# Spark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T

import os, io, uuid, datetime as dt
import numpy as np, pandas as pd
import boto3

S3_BUCKET = "kartikay77-audio-pipeline"   # <-- your bucket
AWS_REGION = "us-east-2"                   # your region (Ohio)
S3_PREFIX  = "audio_pipeline_demo"         # folder in your bucket

# Paste your keys here (or use Colab secrets)
os.environ["AWS_ACCESS_KEY_ID"]     = "AKIA2RB3YVDFTDEUWTVP"
os.environ["AWS_SECRET_ACCESS_KEY"] = "q26929g/NztCl6d3W5fOEn5Mzt0AyG6Q74XTEjmG"
os.environ["AWS_DEFAULT_REGION"]    = AWS_REGION

RAW_PREFIX    = f"s3://{S3_BUCKET}/{S3_PREFIX}/raw"
CURATED_PREFIX= f"s3://{S3_BUCKET}/{S3_PREFIX}/curated"

s3 = boto3.client("s3", region_name=AWS_REGION)
print("Buckets:")
for b in s3.list_buckets()["Buckets"]:
    print(" -", b["Name"])

rng = np.random.default_rng(42)
today = dt.date.today()

# users
n_users = 500
users = pd.DataFrame({
    "user_id": [f"u_{i:05d}" for i in range(n_users)],
    "country": rng.choice(["US","CA","UK","DE","IN","AU"], size=n_users, p=[.35,.1,.15,.1,.25,.05]),
    "signup_date": pd.to_datetime(rng.integers(0, 365, size=n_users), unit="D",
                                  origin=pd.Timestamp(today - dt.timedelta(days=365)))
})

# titles
n_titles = 200
titles = pd.DataFrame({
    "title_id": [f"t_{i:05d}" for i in range(n_titles)],
    "category": rng.choice(["Fiction","Non-Fiction","Self-Help","Tech","Kids","Wellness"], size=n_titles),
    "duration_secs": rng.integers(1800, 36000, size=n_titles)
})

# sessions
n_sessions = 50_000
dates = pd.to_datetime(rng.integers(0, 60, size=n_sessions), unit="D",
                       origin=pd.Timestamp(today - dt.timedelta(days=60)))
sessions = pd.DataFrame({
    "session_id": [str(uuid.uuid4()) for _ in range(n_sessions)],
    "user_id": rng.choice(users["user_id"], size=n_sessions),
    "title_id": rng.choice(titles["title_id"], size=n_sessions),
    "session_date": dates,
    "seconds_listened": rng.integers(60, 12_000, size=n_sessions),
    "device": rng.choice(["iOS","Android","Web","Echo"], size=n_sessions, p=[.4,.35,.2,.05]),
})
# Add some bad rows to test DQ
bad = rng.choice(n_sessions, 200, replace=False)
sessions.loc[bad, "seconds_listened"] = -1

# helper upload
def df_to_s3_csv(df, key):
    buf = io.StringIO(); df.to_csv(buf, index=False)
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=buf.getvalue())

df_to_s3_csv(users,  f"{S3_PREFIX}/raw/users/users.csv")
df_to_s3_csv(titles, f"{S3_PREFIX}/raw/titles/titles.csv")
for d, part in sessions.groupby(sessions["session_date"].dt.date):
    df_to_s3_csv(part, f"{S3_PREFIX}/raw/sessions/dt={d}/sessions.csv")

print("Raw users/titles/sessions uploaded to S3")

# --- Fix Java for Spark in Colab ---
!apt -qq update
!apt -qq install -y openjdk-11-jdk

import os, sys
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

!java -version

# Reinstall PySpark to ensure a clean match
!pip -q install --upgrade pyspark==3.5.1

# Stop any half-started Spark
try:
    spark.stop()
except:
    pass

import os
from pyspark.sql import SparkSession

aws_key = os.environ["AWS_ACCESS_KEY_ID"]
aws_sec = os.environ["AWS_SECRET_ACCESS_KEY"]

# Slightly newer AWS SDK bundle can help avoid jar conflicts
packages = ",".join([
    "org.apache.hadoop:hadoop-aws:3.3.4",
    "com.amazonaws:aws-java-sdk-bundle:1.12.539"
])

spark = (
    SparkSession.builder
    .appName("audio_pipeline_demo")
    .config("spark.jars.packages", packages)
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config("spark.hadoop.fs.s3a.aws.credentials.provider",
            "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
    .config("spark.hadoop.fs.s3a.access.key", aws_key)
    .config("spark.hadoop.fs.s3a.secret.key", aws_sec)
    # Optional stability knobs:
    .config("spark.driver.memory", "2g")
    .config("spark.executor.memory", "2g")
    .getOrCreate()
)

spark

RAW_PREFIX = f"s3a://{S3_BUCKET}/{S3_PREFIX}/raw"
test_df = spark.read.option("header", True).csv(f"{RAW_PREFIX}/users/users.csv")
test_df.show(5)

from pyspark.sql import functions as F

RAW_PREFIX = f"s3a://kartikay77-audio-pipeline/audio_pipeline_demo/raw"
CURATED_PREFIX = f"s3a://kartikay77-audio-pipeline/audio_pipeline_demo/curated"

# read the three raw datasets
raw_users    = spark.read.option("header", True).csv(f"{RAW_PREFIX}/users/users.csv")
raw_titles   = spark.read.option("header", True).csv(f"{RAW_PREFIX}/titles/titles.csv")
raw_sessions = spark.read.option("header", True).csv(f"{RAW_PREFIX}/sessions/*/*.csv")

# convert datatypes and clean negatives
users_s = raw_users.withColumn("signup_date", F.to_date("signup_date"))
titles_s = raw_titles.withColumn("duration_secs", F.col("duration_secs").cast("int"))
sessions_s = (
    raw_sessions
    .withColumn("session_date", F.to_timestamp("session_date"))
    .withColumn("seconds_listened", F.col("seconds_listened").cast("int"))
)

# remove bad sessions (negative durations)
sessions_clean = sessions_s.filter(F.col("seconds_listened") > 0)
print("Cleaned all data successfully!")

fact = (
    sessions_clean
    .join(users_s,  "user_id", "left")
    .join(titles_s, "title_id", "left")
    .withColumn("date", F.to_date("session_date"))
    .withColumn("completed",
                (F.col("seconds_listened") >= F.col("duration_secs") * 0.9).cast("boolean"))
)

dim_users  = users_s.select("user_id", "country", "signup_date").dropDuplicates(["user_id"])
dim_titles = titles_s.select("title_id", "category", "duration_secs").dropDuplicates(["title_id"])
fact_listenings = fact.select(
    "session_id", "user_id", "title_id", "date", "device",
    "seconds_listened", "duration_secs", "completed", "country"
)
print("Joined users, titles, and sessions into curated tables!")

(fact_listenings.repartition(1)
 .write.mode("overwrite").partitionBy("date")
 .parquet(f"{CURATED_PREFIX}/fact_listenings"))

(dim_users.repartition(1)
 .write.mode("overwrite").parquet(f"{CURATED_PREFIX}/dim_users"))

(dim_titles.repartition(1)
 .write.mode("overwrite").parquet(f"{CURATED_PREFIX}/dim_titles"))

print("Curated Parquet data written to S3 successfully!")

# Average listening duration by device and country
(fact_listenings.groupBy("device", "country")
 .agg(F.avg("seconds_listened").alias("avg_secs"))
 .orderBy(F.desc("avg_secs"))
 .show(10, truncate=False))

# Completion rate by content category
(fact.groupBy("category")
 .agg(F.mean(F.col("completed").cast("int")).alias("completion_rate"))
 .orderBy(F.desc("completion_rate"))
 .show(10, truncate=False))

# Daily active users (DAU) for last 14 days
(fact_listenings.filter(F.col("date") >= F.date_sub(F.current_date(), 14))
 .groupBy("date")
 .agg(F.countDistinct("user_id").alias("DAU"))
 .orderBy("date")
 .show(20, truncate=False))

resp = s3.list_objects_v2(Bucket="kartikay77-audio-pipeline", Prefix="audio_pipeline_demo/curated/")
for item in resp.get("Contents", []):
    print(item["Key"])

from pyspark.sql import functions as F


dq_counts = {
    "sessions_nulls": sessions_s.select([
        F.sum(F.col(c).isNull().cast("int")).alias(f"{c}_nulls") for c in sessions_s.columns
    ]).collect()[0].asDict(),
    "users_nulls": users_s.select([
        F.sum(F.col(c).isNull().cast("int")).alias(f"{c}_nulls") for c in users_s.columns
    ]).collect()[0].asDict(),
    "titles_nulls": titles_s.select([
        F.sum(F.col(c).isNull().cast("int")).alias(f"{c}_nulls") for c in titles_s.columns
    ]).collect()[0].asDict(),
}

neg_count = sessions_s.filter(F.col("seconds_listened") <= 0).count()
if neg_count > 0:
    print(f"[DQ] Found {neg_count} invalid sessions with non-positive seconds_listened")

user_keys   = users_s.select("user_id").distinct()
title_keys  = titles_s.select("title_id").distinct()

orphan_user_sessions  = sessions_s.join(user_keys,  "user_id",  "left_anti").count()
orphan_title_sessions = sessions_s.join(title_keys, "title_id", "left_anti").count()

print(f"[DQ] Orphan sessions (no user match): {orphan_user_sessions}")
print(f"[DQ] Orphan sessions (no title match): {orphan_title_sessions}")

users_s  = users_s.dropDuplicates(["user_id"])
titles_s = titles_s.dropDuplicates(["title_id"])

sessions_clean = sessions_s.filter(F.col("seconds_listened") > 0)

# 1) Who am I calling as?
import boto3, re
sts = boto3.client("sts")
who = sts.get_caller_identity()
print("Caller ARN:", who["Arn"])
print("Caller Account:", who["Account"])

# 2) Paste the role ARN you used for the crawler:
iam_role = "arn:aws:iam::<ACCOUNT_ID>:role/<GlueCrawlerRole>"
m = re.match(r"arn:aws:iam::(\d+):role/.+", iam_role)
print("Role Account:", m.group(1) if m else "bad ARN")

import boto3
region     = "us-east-2"
bucket     = "kartikay77-audio-pipeline"
prefix     = "audio_pipeline_demo/curated/"
db_name    = "audio"
crawler    = "audio_curated_crawler"
iam_role   = "arn:aws:iam::723827337419:role/AWSGlueServiceRole-audio"

glue = boto3.client("glue", region_name=region)

# Create DB if missing
try:
    glue.create_database(DatabaseInput={"Name": db_name})
except glue.exceptions.AlreadyExistsException:
    pass

# Create or update crawler
try:
    glue.create_crawler(
        Name=crawler,
        Role=iam_role,
        DatabaseName=db_name,
        Targets={"S3Targets": [{"Path": f"s3://{bucket}/{prefix}"}]},
        TablePrefix="audio_",
        SchemaChangePolicy={
            "UpdateBehavior": "UPDATE_IN_DATABASE",
            "DeleteBehavior": "DEPRECATE_IN_DATABASE"
        }
    )
except glue.exceptions.AlreadyExistsException:
    glue.update_crawler(
        Name=crawler,
        Role=iam_role,
        DatabaseName=db_name,
        Targets={"S3Targets": [{"Path": f"s3://{bucket}/{prefix}"}]},
        TablePrefix="audio_",
    )

glue.start_crawler(Name=crawler)
print("Started Glue crawler.")

import time

while True:
    status = glue.get_crawler(Name=crawler)["Crawler"]["State"]
    print("Crawler status:", status)
    if status == "READY":
        print("Crawler finished successfully.")
        break
    time.sleep(10)

tables = glue.get_tables(DatabaseName=db_name)["TableList"]
for t in tables:
    print("-", t["Name"])

athena = boto3.client("athena", region_name=region)

query = """
SELECT category, COUNT(*) AS num_titles, ROUND(AVG(duration_secs), 2) AS avg_duration
FROM audio.audio_dim_titles
GROUP BY category
ORDER BY num_titles DESC
LIMIT 10;
"""

output = f"s3://{bucket}/athena-results/"

response = athena.start_query_execution(
    QueryString=query,
    QueryExecutionContext={"Database": db_name},
    ResultConfiguration={"OutputLocation": output},
)

print("Query started with ID:", response["QueryExecutionId"])

qid = response["QueryExecutionId"]

while True:
    state = athena.get_query_execution(QueryExecutionId=qid)["QueryExecution"]["Status"]["State"]
    if state in ["SUCCEEDED", "FAILED", "CANCELLED"]:
        print("Athena query state:", state)
        break
    time.sleep(3)

if state == "SUCCEEDED":
    results = athena.get_query_results(QueryExecutionId=qid)
    for row in results["ResultSet"]["Rows"]:
        print(row)

import boto3, time, s3fs, pandas as pd

region  = "us-east-2"
bucket  = "kartikay77-audio-pipeline"
db_name = "audio"

athena = boto3.client("athena", region_name=region)

sql = """
SELECT
  f.seconds_listened,
  f.duration_secs,
  CAST(f.seconds_listened * 1.0 / NULLIF(f.duration_secs, 0) AS DOUBLE) AS listen_ratio,
  CAST(f.completed AS INTEGER) AS label,       -- <-- changed here
  f.device,
  f.country,
  t.category
FROM audio.audio_fact_listenings f
JOIN audio.audio_dim_titles t USING (title_id)
WHERE f.duration_secs IS NOT NULL
"""

out = f"s3://{bucket}/athena-results/"

resp = athena.start_query_execution(
    QueryString=sql,
    QueryExecutionContext={'Database': db_name},
    ResultConfiguration={'OutputLocation': out},
)
qid = resp["QueryExecutionId"]

# wait
while True:
    state = athena.get_query_execution(QueryExecutionId=qid)["QueryExecution"]["Status"]["State"]
    if state in ("SUCCEEDED","FAILED","CANCELLED"):
        print("Athena:", state); break
    time.sleep(2)

# read result if succeeded
if state == "SUCCEEDED":
    import s3fs
    fs = s3fs.S3FileSystem()
    with fs.open(f"{bucket}/athena-results/{qid}.csv") as f:
        df = pd.read_csv(f)
    print(df.head())

print(df.shape)
print(df.dtypes)
df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report

# Keep only non-leaky features (categorical)
X = df[['device', 'country', 'category']].copy()
y = df['label'].astype(int)

cat_cols = ['device', 'country', 'category']

pre = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ],
    remainder='drop'
)

clf = Pipeline([
    ('prep', pre),
    ('model', LogisticRegression(max_iter=200, class_weight='balanced'))
])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

clf.fit(X_train, y_train)
y_pred = clf.predict_proba(X_test)[:, 1]

print("AUC:", roc_auc_score(y_test, y_pred))
print(classification_report(y_test, y_pred > 0.5))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, color='royalblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0,1],[0,1],'--',color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

from sklearn.metrics import precision_recall_curve

prec, rec, _ = precision_recall_curve(y_test, y_pred)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, color='orange', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(alpha=0.3)
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, y_pred > 0.5)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()